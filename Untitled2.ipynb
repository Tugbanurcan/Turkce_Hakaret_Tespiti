{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Op51xIAntk"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPBsvsZmC82a",
        "outputId": "64773553-700e-4e87-b7e8-a5389567a6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/YeniNlp/veri_seti.xlsx\"\n"
      ],
      "metadata": {
        "id": "I3pITmOPDDHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnsjUqd6COR0",
        "outputId": "901ef56a-30cf-472a-f24c-92d14b310255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılan cihaz: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute_loss Fonksiyonu (MultiLabelTrainer sınıfında):\n",
        "\n",
        "Bu fonksiyon, çok etiketli sınıflandırma (multi-label classification) için özel olarak tanımlanmıştır. Modelin çıkışı (logits) ile gerçek etiketler (labels) arasındaki farkı BCEWithLogitsLoss kullanarak hesaplar.\n",
        "\n",
        "Eğer sınıf dengesizliği varsa, her sınıfa özel ağırlıklar (class weights) uygulanır. Böylece az örneğe sahip sınıflar model tarafından ihmal edilmez.\n"
      ],
      "metadata": {
        "id": "fZ9Y5M8sYSCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EvalPrediction\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    hamming_loss,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU kontrolü\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "# ========== 1. VERİ YÜKLEME VE ÖN İŞLEME ==========\n",
        "\n",
        "class ContentModerationDataset(Dataset):\n",
        "    \"\"\"Multi-label sınıflandırma için custom dataset\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(labels)\n",
        "        }\n",
        "\n",
        "def load_and_prepare_data(file_path):\n",
        "    \"\"\"Excel dosyasından veri yükleme ve hazırlama\"\"\"\n",
        "    print(\"Veri yükleniyor...\")\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Metin ve etiket sütunlarını ayır\n",
        "    text_column = 'Paylaşım'\n",
        "    label_columns = ['Tespit', 'Küfür', 'Tehdit', 'Hakaret',\n",
        "                     'Dolandırıcılık', 'Cinsel İçerik', 'Irkçılık']\n",
        "\n",
        "    texts = df[text_column].values\n",
        "    labels = df[label_columns].values\n",
        "\n",
        "    # Veri istatistikleri\n",
        "    print(f\"\\nToplam örnek sayısı: {len(texts)}\")\n",
        "    print(\"\\nSınıf dağılımları:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        positive_ratio = labels[:, i].sum() / len(labels) * 100\n",
        "        print(f\"{col}: {labels[:, i].sum()} pozitif ({positive_ratio:.2f}%)\")\n",
        "\n",
        "    return texts, labels, label_columns\n",
        "\n",
        "def calculate_class_weights(labels):\n",
        "    \"\"\"Dengesiz veri için class weight hesaplama\"\"\"\n",
        "    class_weights = []\n",
        "    for i in range(labels.shape[1]):\n",
        "        # Her sınıf için pozitif/negatif oranına göre weight hesapla\n",
        "        pos_weight = (labels.shape[0] - labels[:, i].sum()) / labels[:, i].sum()\n",
        "        class_weights.append(pos_weight)\n",
        "    return torch.FloatTensor(class_weights)\n",
        "\n",
        "# ========== 2. MODEL HAZIRLAMA ==========\n",
        "\n",
        "def setup_model_and_tokenizer(model_name=\"dbmdz/bert-base-turkish-cased\", num_labels=7):   #Modelin num_labels=7 parametresi ile çoklu etiketli sınıflandırma yapması sağlandı.\n",
        "    \"\"\"Model ve tokenizer hazırlama\"\"\"\n",
        "    print(f\"\\nModel yükleniyor: {model_name}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)                   #Tokenizer, metni küçük parçalara (token) ayırarak her birine sayısal bir ID atar ve BERT gibi modeller bu sayısal ID’lerle çalışır.\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "# ========== 3. CUSTOM TRAINER ==========\n",
        "\n",
        "class MultiLabelTrainer(Trainer):\n",
        "    \"\"\"Class weight destekli custom trainer\"\"\"\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Weighted BCE Loss\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.class_weights.to(self.args.device))\n",
        "        else:\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NW5RQ7HCpo4",
        "outputId": "28ae7800-417d-417d-ea0f-8a4f9778bbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılan cihaz: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 4. EVALUATION METRICS ==========\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"Multi-label metrikler hesaplama\"\"\"\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    # Sigmoid uygula ve threshold (0.5)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    predictions = np.zeros(probs.shape)\n",
        "    predictions[np.where(probs >= 0.5)] = 1\n",
        "\n",
        "    # Metrikler\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
        "    hamming = hamming_loss(labels, predictions)\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None\n",
        "    )\n",
        "\n",
        "    # ROC-AUC (eğer mümkünse)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, average='macro')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    return {\n",
        "        'macro_f1': macro_f1,\n",
        "        'micro_f1': micro_f1,\n",
        "        'hamming_loss': hamming,\n",
        "        'roc_auc': roc_auc,\n",
        "        'per_class_f1': f1.tolist()\n",
        "    }\n",
        "\n",
        "# ========== 5. ANA EĞİTİM FONKSİYONU ==========\n",
        "def train_model(\n",
        "    train_texts, train_labels,\n",
        "    val_texts, val_labels,\n",
        "    label_columns,\n",
        "    model_name=\"dbmdz/bert-base-turkish-cased\",\n",
        "    batch_size=16,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    max_length=128,\n",
        "    output_dir=\"./turkish_content_classifier\"\n",
        "):\n",
        "    \"\"\"Model eğitim ana fonksiyonu\"\"\"\n",
        "\n",
        "    # Model ve tokenizer hazırla\n",
        "    model, tokenizer = setup_model_and_tokenizer(model_name, len(label_columns))\n",
        "\n",
        "    # Dataset oluştur\n",
        "    train_dataset = ContentModerationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = ContentModerationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    # Class weights hesapla\n",
        "    class_weights = calculate_class_weights(train_labels)\n",
        "    print(f\"\\nClass weights: {class_weights.tolist()}\")\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        fp16=torch.cuda.is_available(),  # Mixed precision training\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # Trainer oluştur\n",
        "    trainer = MultiLabelTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Eğitimi başlat\n",
        "    print(\"\\nEğitim başlıyor...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # En iyi modeli kaydet\n",
        "    trainer.save_model(f\"{output_dir}/best_model\")\n",
        "    tokenizer.save_pretrained(f\"{output_dir}/best_model\")\n",
        "\n",
        "    return trainer, model, tokenizer\n",
        "\n",
        "# ========== 6. TEST VE DEĞERLENDİRME ==========\n",
        "def evaluate_model(model, tokenizer, test_texts, test_labels, label_columns, max_length=128):\n",
        "    \"\"\"Test seti üzerinde model değerlendirme\"\"\"\n",
        "    print(\"\\nModel test ediliyor...\")\n",
        "\n",
        "    test_dataset = ContentModerationDataset(test_texts, test_labels, tokenizer, max_length)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.sigmoid(outputs.logits).cpu()\n",
        "\n",
        "            all_predictions.extend(predictions.numpy())\n",
        "            all_labels.extend(labels.numpy())"
      ],
      "metadata": {
        "id": "zBoO5eTxJL2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ========== 4. DEĞERLENDİRME METRİKLERİ ==========\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"Çok etiketli metrikleri hesaplama\"\"\"\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    # Sigmoid uygula ve eşik değeri (0.5)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    predictions = np.zeros(probs.shape)\n",
        "    predictions[np.where(probs >= 0.5)] = 1\n",
        "\n",
        "    # Metrikler\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
        "    hamming = hamming_loss(labels, predictions)\n",
        "\n",
        "    # Sınıf bazlı metrikler\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None\n",
        "    )\n",
        "\n",
        "    # ROC-AUC (mümkünse)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, average='macro')\n",
        "    except:\n",
        "        roc_auc = 0.0 # Hata durumunda varsayılan değer\n",
        "\n",
        "    return {\n",
        "        'macro_f1': macro_f1,\n",
        "        'micro_f1': micro_f1,\n",
        "        'hamming_loss': hamming,\n",
        "        'roc_auc': roc_auc,\n",
        "        'per_class_f1': f1.tolist()\n",
        "    }\n",
        "\n",
        "# ========== 5. ANA EĞİTİM FONKSİYONU ==========\n",
        "def train_model(\n",
        "    train_texts, train_labels,\n",
        "    val_texts, val_labels,\n",
        "    label_columns,\n",
        "    model_name=\"dbmdz/bert-base-turkish-cased\",\n",
        "    batch_size=16,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    max_length=128,\n",
        "    output_dir=\"./turkish_content_classifier\"\n",
        "):\n",
        "    \"\"\"Model eğitim ana fonksiyonu\"\"\"\n",
        "\n",
        "    # Model ve tokenizer hazırla\n",
        "    model, tokenizer = setup_model_and_tokenizer(model_name, len(label_columns))\n",
        "\n",
        "    # Dataset oluştur\n",
        "    train_dataset = ContentModerationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = ContentModerationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    # Sınıf ağırlıklarını hesapla\n",
        "    class_weights = calculate_class_weights(train_labels)\n",
        "    print(f\"\\nSınıf ağırlıkları: {class_weights.tolist()}\")\n",
        "\n",
        "    # Eğitim argümanları\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\", # veya \"steps\", \"no\"\n",
        "        save_strategy=\"epoch\", # veya \"steps\", \"no\"\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2, # Sadece en iyi N modeli kaydet\n",
        "        fp16=torch.cuda.is_available(),  # Mixed precision training (varsa)\n",
        "        dataloader_num_workers=2, # Veri yükleme için worker sayısı\n",
        "        remove_unused_columns=False, # Dataset'ten kullanılmayan sütunları kaldırma\n",
        "    )\n",
        "\n",
        "    # Trainer oluştur\n",
        "    trainer = MultiLabelTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Eğitimi başlat\n",
        "    print(\"\\nEğitim başlıyor...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # En iyi modeli kaydet\n",
        "    trainer.save_model(f\"{output_dir}/best_model\")\n",
        "    tokenizer.save_pretrained(f\"{output_dir}/best_model\")\n",
        "\n",
        "    return trainer, model, tokenizer\n",
        "\n",
        "# ========== 6. TEST VE DEĞERLENDİRME ==========\n",
        "def evaluate_model(model, tokenizer, test_texts, test_labels, label_columns, max_length=128):\n",
        "    \"\"\"Test seti üzerinde model değerlendirme\"\"\"\n",
        "    print(\"\\nModel test ediliyor...\")\n",
        "\n",
        "    test_dataset = ContentModerationDataset(test_texts, test_labels, tokenizer, max_length)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    model.eval() # Modeli değerlendirme moduna al\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad(): # Gradyan hesaplamayı kapat\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'] # Etiketleri CPU'da bırak\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.sigmoid(outputs.logits).cpu() # Tahminleri CPU'ya taşı ve sigmoid uygula\n",
        "\n",
        "            all_predictions.extend(predictions.numpy())\n",
        "            all_labels.extend(labels.numpy()) # Etiketleri numpy'a çevir\n",
        "\n",
        "    # İkinci hücreden taşınan kod, doğru girinti ile bu fonksiyonun içine eklendi\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Eşik değeri optimizasyonu (isteğe bağlı)\n",
        "    best_thresholds = []\n",
        "    for i in range(len(label_columns)):\n",
        "        best_threshold = 0.5 # Başlangıç eşiği\n",
        "        best_f1 = 0 # Başlangıç F1 skoru\n",
        "\n",
        "        for threshold in np.arange(0.1, 0.9, 0.05): # Farklı eşik değerlerini dene\n",
        "            preds = (all_predictions[:, i] >= threshold).astype(int) # Belirli eşiğe göre tahminleri ikili yap\n",
        "            f1 = f1_score(all_labels[:, i], preds) # F1 skorunu hesapla\n",
        "            if f1 > best_f1: # Daha iyi bir F1 skoru bulduysak güncelle\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        best_thresholds.append(best_threshold) # En iyi eşiği kaydet\n",
        "\n",
        "    print(f\"\\nOptimal eşik değerleri: {best_thresholds}\")\n",
        "\n",
        "    # Optimize edilmiş eşik değerleri ile nihai tahminler\n",
        "    final_predictions = np.zeros_like(all_predictions)\n",
        "    for i in range(len(label_columns)):\n",
        "        final_predictions[:, i] = (all_predictions[:, i] >= best_thresholds[i]).astype(int)\n",
        "\n",
        "    # Detaylı rapor\n",
        "    print(\"\\n========== TEST SONUÇLARI ==========\")\n",
        "    print(f\"Macro F1: {f1_score(all_labels, final_predictions, average='macro'):.4f}\")\n",
        "    print(f\"Micro F1: {f1_score(all_labels, final_predictions, average='micro'):.4f}\")\n",
        "    print(f\"Hamming Loss: {hamming_loss(all_labels, final_predictions):.4f}\")\n",
        "\n",
        "    print(\"\\nSınıf bazlı sonuçlar:\")\n",
        "    for i, label in enumerate(label_columns):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels[:, i], final_predictions[:, i], average='binary'\n",
        "        )\n",
        "        print(f\"\\n{label}:\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "    return best_thresholds\n",
        "\n",
        "# ========== 7. TAHMİN FONKSİYONU ==========\n",
        "def predict_text(text, model, tokenizer, label_columns, thresholds=None):\n",
        "    \"\"\"Tek bir metin için tahmin\"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = [0.5] * len(label_columns) # Eşik değerleri belirtilmemişse varsayılan 0.5 kullan\n",
        "\n",
        "    model.eval() # Modeli değerlendirme moduna al\n",
        "\n",
        "    # Tokenize et\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128, # Eğitim sırasında kullanılan max_length ile aynı\n",
        "        return_tensors='pt'\n",
        "    ).to(device) # Girişleri cihaza (GPU/CPU) taşı\n",
        "\n",
        "    # Tahmin yap\n",
        "    with torch.no_grad(): # Gradyan hesaplamayı kapat\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits).cpu().numpy()[0] # Çıktılara sigmoid uygula, CPU'ya taşı ve numpy'a çevir\n",
        "\n",
        "    # Sonuçları yorumla\n",
        "    results = {}\n",
        "    for i, (label, threshold) in enumerate(zip(label_columns, thresholds)):\n",
        "        results[label] = {\n",
        "            'prediction': int(predictions[i] >= threshold), # Eşik değerine göre ikili tahmin (0 veya 1)\n",
        "            'confidence': float(predictions[i]) # Olasılık değeri (güven)\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ========== 8. ANA ÇALIŞTIRMA KODU ==========\n",
        "# __name__ == \"__main__\" bloğu, normalde bir betiğin en son yürütülebilir kısmı olmalıdır,\n",
        "# ancak bir notebook'ta ana yürütme akışını başlatmak istediğiniz yere koyabilirsiniz.\n",
        "if __name__ == \"__main__\": # __name__ değişkenini düzeltme\n",
        "    # Veri yükle\n",
        "    # 'veri_seti.xlsx' dosyasının erişilebilir olduğundan emin olun, örneğin Google Drive'dan bağlanmış olmalı\n",
        "    texts, labels, label_columns = load_and_prepare_data('/content/drive/MyDrive/YeniNlp/veri_seti.xlsx') # file_path değişkenini kullanma\n",
        "\n",
        "    # Train/val/test split\n",
        "    # Stratify'nin, pozitif örneği olmayan sütunlar için tek bir sütun üzerinde yapıldığından emin olun.\n",
        "    # labels[:, 0] kullanmak, ilk etiket sütununun ('Tespit') stratifikasyon için uygun olduğunu varsayar.\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        texts, labels, test_size=0.15, random_state=42, stratify=labels[:, 0]\n",
        "    )\n",
        "\n",
        "    # Doğrulama setini eğitim setinden ayır\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp[:, 0] # 0.15 / (1 - 0.15) ≈ 0.176\n",
        "    )\n",
        "\n",
        "    print(f\"\\nVeri bölünmesi:\")\n",
        "    print(f\"Eğitim: {len(X_train)} örnek\")\n",
        "    print(f\"Doğrulama: {len(X_val)} örnek\")\n",
        "    print(f\"Test: {len(X_test)} örnek\")\n",
        "\n",
        "    # Model eğit\n",
        "    trainer, model, tokenizer = train_model(\n",
        "        X_train, y_train,\n",
        "        X_val, y_val,\n",
        "        label_columns,\n",
        "        model_name=\"dbmdz/bert-base-turkish-cased\",  # veya \"loodos/bert-base-turkish-uncased\"\n",
        "        batch_size=16,\n",
        "        num_epochs=3,\n",
        "        learning_rate=2e-5,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Test et ve optimal eşik değerlerini bul\n",
        "    best_thresholds = evaluate_model(model, tokenizer, X_test, y_test, label_columns)\n",
        "\n",
        "    # Örnek tahmin\n",
        "    print(\"\\n========== ÖRNEK TAHMİNLER ==========\")\n",
        "    test_texts = [\n",
        "        \"Bu ürün harika, herkese tavsiye ederim!\",\n",
        "        \"Seni tehdit ediyorum, dikkatli ol!\",\n",
        "        \"Kredi kartı bilgilerinizi gönderin, hediye kazanın!\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        print(f\"\\nMetin: {text}\")\n",
        "        results = predict_text(text, model, tokenizer, label_columns, best_thresholds)\n",
        "        for label, result in results.items():\n",
        "            if result['prediction'] == 1:\n",
        "                # Güven oranını yüzde formatında yazdırma\n",
        "                print(f\"  ✓ {label}: {result['confidence']:.2%}\")\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FzXbsySsXQhz",
        "outputId": "d5933ce0-634c-4819-e9e4-0cfad590a717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri yükleniyor...\n",
            "\n",
            "Toplam örnek sayısı: 9999\n",
            "\n",
            "Sınıf dağılımları:\n",
            "Tespit: 4731 pozitif (47.31%)\n",
            "Küfür: 2015 pozitif (20.15%)\n",
            "Tehdit: 339 pozitif (3.39%)\n",
            "Hakaret: 2357 pozitif (23.57%)\n",
            "Dolandırıcılık: 501 pozitif (5.01%)\n",
            "Cinsel İçerik: 798 pozitif (7.98%)\n",
            "Irkçılık: 647 pozitif (6.47%)\n",
            "\n",
            "Veri bölünmesi:\n",
            "Eğitim: 7003 örnek\n",
            "Doğrulama: 1496 örnek\n",
            "Test: 1500 örnek\n",
            "\n",
            "Model yükleniyor: dbmdz/bert-base-turkish-cased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sınıf ağırlıkları: [1.1137940883636475, 3.9247539043426514, 28.54852294921875, 3.2727272510528564, 18.726760864257812, 11.372791290283203, 14.666666984558105]\n",
            "\n",
            "Eğitim başlıyor...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1314/1314 03:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Micro F1</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>Roc Auc</th>\n",
              "      <th>Per Class F1</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.376800</td>\n",
              "      <td>0.384067</td>\n",
              "      <td>0.707551</td>\n",
              "      <td>0.743005</td>\n",
              "      <td>0.104374</td>\n",
              "      <td>0.966207</td>\n",
              "      <td>[0.906721536351166, 0.7459727385377943, 0.5157232704402516, 0.7324613555291319, 1.0, 0.3712784588441331, 0.6807017543859649]</td>\n",
              "      <td>2.981400</td>\n",
              "      <td>501.778000</td>\n",
              "      <td>31.529000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.365500</td>\n",
              "      <td>0.242458</td>\n",
              "      <td>0.842089</td>\n",
              "      <td>0.853672</td>\n",
              "      <td>0.051757</td>\n",
              "      <td>0.980522</td>\n",
              "      <td>[0.9304964539007092, 0.8700729927007299, 0.7924528301886793, 0.7666263603385731, 1.0, 0.64, 0.8949771689497716]</td>\n",
              "      <td>2.946500</td>\n",
              "      <td>507.726000</td>\n",
              "      <td>31.903000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.165600</td>\n",
              "      <td>0.236864</td>\n",
              "      <td>0.866920</td>\n",
              "      <td>0.874132</td>\n",
              "      <td>0.043258</td>\n",
              "      <td>0.980923</td>\n",
              "      <td>[0.9298998569384835, 0.8766328011611031, 0.8118811881188119, 0.7924528301886793, 1.0, 0.7575757575757576, 0.9]</td>\n",
              "      <td>2.949900</td>\n",
              "      <td>507.134000</td>\n",
              "      <td>31.865000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"[0.906721536351166, 0.7459727385377943, 0.5157232704402516, 0.7324613555291319, 1.0, 0.3712784588441331, 0.6807017543859649]\" of type <class 'list'> for key \"eval/per_class_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.9304964539007092, 0.8700729927007299, 0.7924528301886793, 0.7666263603385731, 1.0, 0.64, 0.8949771689497716]\" of type <class 'list'> for key \"eval/per_class_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.9298998569384835, 0.8766328011611031, 0.8118811881188119, 0.7924528301886793, 1.0, 0.7575757575757576, 0.9]\" of type <class 'list'> for key \"eval/per_class_f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model test ediliyor...\n",
            "\n",
            "Optimal eşik değerleri: [np.float64(0.25000000000000006), np.float64(0.8500000000000002), np.float64(0.8500000000000002), np.float64(0.8000000000000002), np.float64(0.20000000000000004), np.float64(0.8500000000000002), np.float64(0.8000000000000002)]\n",
            "\n",
            "========== TEST SONUÇLARI ==========\n",
            "Macro F1: 0.8970\n",
            "Micro F1: 0.8953\n",
            "Hamming Loss: 0.0340\n",
            "\n",
            "Sınıf bazlı sonuçlar:\n",
            "\n",
            "Tespit:\n",
            "  Precision: 0.9102\n",
            "  Recall: 0.9563\n",
            "  F1-Score: 0.9327\n",
            "\n",
            "Küfür:\n",
            "  Precision: 0.8723\n",
            "  Recall: 0.8566\n",
            "  F1-Score: 0.8644\n",
            "\n",
            "Tehdit:\n",
            "  Precision: 0.8852\n",
            "  Recall: 0.9153\n",
            "  F1-Score: 0.9000\n",
            "\n",
            "Hakaret:\n",
            "  Precision: 0.8630\n",
            "  Recall: 0.8110\n",
            "  F1-Score: 0.8362\n",
            "\n",
            "Dolandırıcılık:\n",
            "  Precision: 1.0000\n",
            "  Recall: 0.9875\n",
            "  F1-Score: 0.9937\n",
            "\n",
            "Cinsel İçerik:\n",
            "  Precision: 0.8036\n",
            "  Recall: 0.7759\n",
            "  F1-Score: 0.7895\n",
            "\n",
            "Irkçılık:\n",
            "  Precision: 0.9890\n",
            "  Recall: 0.9375\n",
            "  F1-Score: 0.9626\n",
            "\n",
            "========== ÖRNEK TAHMİNLER ==========\n",
            "\n",
            "Metin: Bu ürün harika, herkese tavsiye ederim!\n",
            "  ✓ Tespit: 97.91%\n",
            "\n",
            "Metin: Seni tehdit ediyorum, dikkatli ol!\n",
            "  ✓ Tehdit: 99.05%\n",
            "\n",
            "Metin: Kredi kartı bilgilerinizi gönderin, hediye kazanın!\n",
            "  ✓ Tespit: 79.10%\n",
            "  ✓ Dolandırıcılık: 74.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzXHudB36Nr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 9. KAYDEDİLMİŞ MODELİ YÜKLEME VE KULLANMA ==========\n",
        "\n",
        "def load_model_and_tokenizer(model_path, device):\n",
        "    \"\"\"Kaydedilmiş modeli ve tokenizer'ı yükleme\"\"\"\n",
        "    print(f\"\\nModel yükleniyor: {model_path}\")\n",
        "    # Tokenizer'ı yükle\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    # Modeli yükle\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Örneğin, ana çalıştırma bloğunuzun sonuna ekleyebilirsiniz\n",
        "if __name__ == \"__main__\":\n",
        "    # ... (önceki veri yükleme ve eğitim kodları) ...\n",
        "\n",
        "    # Model eğitildikten ve kaydedildikten sonra\n",
        "    output_directory = \"./turkish_content_classifier\" # train_model fonksiyonunda kullanılan output_dir ile aynı\n",
        "    saved_model_path = f\"{output_directory}/best_model\" # Kaydedilen modelin tam yolu\n",
        "\n",
        "    # Kaydedilmiş modeli ve tokenizer'ı yükle\n",
        "    loaded_model, loaded_tokenizer = load_model_and_tokenizer(saved_model_path, device)\n",
        "\n",
        "    # Yüklenen model ile örnek tahmin yapma (isteğe bağlı)\n",
        "    print(\"\\n========== YÜKLENEN MODEL İLE ÖRNEK TAHMİNLER ==========\")\n",
        "    test_texts_loaded = [\n",
        "        \"Yeni yüklenen modelle test metni 1\",\n",
        "        \"Yeni yüklenen modelle test metni 2\"\n",
        "    ]\n",
        "    # Best thresholds değerlerini evaluate_model fonksiyonundan almanız veya kaydetmeniz gerekir.\n",
        "    # Basitlik için burada varsayılan 0.5 kullanıldı veya evaluate_model'den dönen değeri kullanın.\n",
        "    # best_thresholds = evaluate_model(model, tokenizer, X_test, y_test, label_columns) # Daha önce çalıştırıldıysa bu değeri kullanın\n",
        "\n",
        "    for text in test_texts_loaded:\n",
        "        print(f\"\\nMetin (Yüklendi): {text}\")\n",
        "        # predict_text fonksiyonunu yüklenen model ve tokenizer ile kullanın\n",
        "        results_loaded = predict_text(text, loaded_model, loaded_tokenizer, label_columns, best_thresholds if 'best_thresholds' in locals() else None)\n",
        "        for label, result in results_loaded.items():\n",
        "            if result['prediction'] == 1:\n",
        "                print(f\"  ✓ {label}: {result['confidence']:.2%}\")"
      ],
      "metadata": {
        "id": "hGSLj0TFZ2Uh",
        "outputId": "a16c68c9-bb27-4250-9f40-227d8e25fd4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model yükleniyor: ./turkish_content_classifier/best_model\n",
            "\n",
            "========== YÜKLENEN MODEL İLE ÖRNEK TAHMİNLER ==========\n",
            "\n",
            "Metin (Yüklendi): Yeni yüklenen modelle test metni 1\n",
            "  ✓ Tespit: 97.40%\n",
            "\n",
            "Metin (Yüklendi): Yeni yüklenen modelle test metni 2\n",
            "  ✓ Tespit: 97.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_metrics(trainer):\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    train_loss = []\n",
        "    eval_loss = []\n",
        "    eval_macro_f1 = []\n",
        "\n",
        "    epochs = []\n",
        "\n",
        "    for entry in log_history:\n",
        "        if 'loss' in entry and 'epoch' in entry:\n",
        "            train_loss.append(entry['loss'])\n",
        "            epochs.append(entry['epoch'])\n",
        "        if 'eval_loss' in entry:\n",
        "            eval_loss.append(entry['eval_loss'])\n",
        "        if 'eval_macro_f1' in entry:\n",
        "            eval_macro_f1.append(entry['eval_macro_f1'])\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Kayıp grafiği\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, label='Training Loss')\n",
        "    # eval_loss bazen farklı sayıda olabilir, onu epoch ile eşleştirmek gerekebilir\n",
        "    plt.plot(range(1, len(eval_loss)+1), eval_loss, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Metrik grafiği (Macro F1)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(eval_macro_f1)+1), eval_macro_f1, label='Validation Macro F1')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Macro F1 Score')\n",
        "    plt.title('Validation Macro F1 Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kHR_NzwY4H2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}